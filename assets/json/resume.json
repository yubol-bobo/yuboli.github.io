{
  "basics": {
    "name": "Yubo Li",
    "image": "",
    "email": "yubol@andrew.cmu.edu",
    "location": {
      "postalCode": "PA 15213",
      "city": "Pittsburgh",
      "countryCode": "US",
      "region": "PA"
    },
    "profiles": [
      {
        "network": "Twitter",
        "username": "AlbertEinstein",
        "url": "https://twitter.com/AlbertEinstein"
      }
    ]
  },
  "education": [
    {
      "institution": "Carnegie Mellon University",
      "location": "Pittsburgh, PA",
      "url": "https://www.cmu.edu/",
      "area": "Information Systems",
      "studyType": "Ph.D.",
      "startDate": "2022-08-29",
      "endDate": "now",
      "score": "10"
    },
    {
      "institution": "Carnegie Mellon University",
      "location": "Pittsburgh, PA",
      "url": "https://www.cmu.edu/",
      "area": "Information Systems",
      "studyType": "M.S.",
      "startDate": "2020-08-31",
      "endDate": "2021-12-31",
      "score": "10"
    },
    {
      "institution": "University of California, San Diego",
      "location": "La Jolla, CA",
      "url": "https://www.ucsd.edu/",
      "area": "Applied Mathematics | Business",
      "studyType": "B.S.",
      "startDate": "2015-09-15",
      "endDate": "2019-06-15",
      "score": "10"
    }
  ],
  "work": [
    {
      "name": "Amazon",
      "position": "Applied Scientist Intern",
      "startDate": "2025-06-02",
      "endDate": "2025-08-22",
      "summary": "Led end-to-end development of a generative recommendation system by pre-training a custom user encoder on 172k interaction sequences and progressively aligning it with LLMs, achieving strong gains over prompting baselines (Hit@3 60.1%, NDCG@3 67.5%) with 93.4% fewer tokens and 64.8% lower cost. Built a production-ready RAG/search pipeline with FAISS HNSW over 60k user embeddings and implemented distributed multi-GPU training with FSDP and BF16 mixed precision to scale training and inference efficiently."
    },
    {
      "name": "HireBeat",
      "position": "Data Scientist",
      "startDate": "2019-10-02",
      "endDate": "2020-03-22",
      "summary": "Leveraged machine learning and NLP techniques to develop a data analysis pipeline that optimized content recommendation algorithms, significantly improving personalization accuracy and user engagement."
    },
    {
      "name": "R-Eat",
      "position": "Founder",
      "startDate": "2012-07-02",
      "endDate": "2014-05-22",
      "summary": "Entrepreneurship: Founded and grew an online food delivery business by enhancing customer experience through UI optimization and personalized marketing, resulting in a 60% sales increase and a loyal customer base of over 2,000."
    }
  ],
  
  "awards": [
    {
      "title": "TCS Presidential Fellowship",
      "date": "2025",
      "awarder": "Carnegie Mellon University"
    },
    {
      "title": "CMLH Fellowship in Generative AI in Healthcare",
      "date": "2024",
      "awarder": "Center for Machine Learning and Health, Carnegie Mellon University",
      "url": "https://mlh.cmu.edu/fellowships/generative-ai-in-healthcare-fellowship/"
    },
    {
      "title": "CMU Heinz College Research Excellence Award",
      "date": "2023",
      "awarder": "Center for Machine Learning and Health, Carnegie Mellon University",
      "url": "https://www.cs.cmu.edu/cmlh/digital-health-archive/cmlh-digital-health-fellows-2023"
    }
  ],
  "skills": [
    {
      "name": "Programming Languages",
      "level": "Master",
      "icon": "fa-solid fa-hashtag",
      "keywords": [
        "Python",
        "Java",
        "SQL",
        "R"
      ]
    },
    {
      "name": "Technologies & Frameworks",
      "level": "Master",
      "icon": "fa-solid fa-gears",
      "keywords": [
        "PyTorch",
        "TensorFlow",
        "scikit-learn",
        "Pandas",
        "NumPy",
        "AWS"
      ]
    },
    {
      "name": "Large Language Models",
      "level": "Master",
      "icon": "fa-solid fa-brain ",
      "keywords": [
        "Prompting Optimization",
        "RAG",
        "Fine-tuning",
        "LoRA",
        "RLHF",
        "Model Distillation",
        "Evaluation Metrics",
        "Parallel Training",
        "Agentic Frameworks"
      ]
    },
    {
      "name": "Machine Learning & Deep Learning",
      "level": "Master",
      "icon": "fa-solid fa-chart-line",  
      "keywords": [
        "Supervised Learning",
        "Unsupervised Learning",
        "Predictive Modeling",
        "Natural Language Processing",
        "Time Series Analysis",
        "Model Interpretability"
      ]}
  ],
  
  "languages": [
    {
      "language": "Mandarin",
      "fluency": "Native speaker",
      "icon": ""
    },
    {
      "language": "English",
      "fluency": "Fluent",
      "icon": ""
    }
  ],
  "projects": [
    {
      "name": "Agentic Q&A Systems for Trustworthy Organ Transplantation Guidance",
      "highlights": ["Q&A System: Developed Q&A systems for organ transplantation patient care by integrating retrieval-augmented generation (RAG) with adaptive frameworks that synthesize patient-specific medical history, clinical guidelines, and dynamic risk factors to deliver tailored guidance and support.",
          "RAG/Retriever: Engineered a hybrid retrieval pipeline combining both sparse and dense retrievers (DPR, ANCE) to accurately extract and verify medical content from over 100 transplant-specific handbooks.",
          "LLM Reasoning Framework: Developed an incentivized reasoning mechanism that optimizes LLM performance by rewarding multi-step chain-of-question sequences, enabling the system to reason freely and proactively request critical diagnostic information."],
      "startDate": "2025-01-01",
      "endDate": "Present"
    },
    {
      "name": "LLM Alignment & Multi-Turn Consistency",
      "highlights": [
        "Distributed Training: Architected and optimized distributed training infrastructure for LLMs (Mistral, LLaMA, Deepseek) across 4 nodes with 16 NVIDIA GH200 GPUs, implementing tensor parallelism, pipeline parallelism, and ZeRO optimizer techniques to efficiently scale model training and reduce computational overhead.",
        "Fine-tuning: Performed fine-tuning of state-of-the-art LLMs both locally (on-premises clusters) and via OpenAI's official GPT-4o API, successfully improving model consistency, confidence-aware reasoning, and achieving state-of-the-art performance.",
        "Evaluation: Proposed a metric to quantitatively assess multi-turn response stability, prioritizing early interaction accuracy and swift recovery from initial errors. Released a comprehensive benchmark to test the model's ability to sustain coherent and contextually accurate responses during dynamic, multi-turn conversations."
      ],
      "startDate": "2024-05-01",
      "endDate": "Present"
    },
    {
      "name": "Chronic Kidney Disease Progression Analysis with DL & XAI",
      "highlights":[
        "Deep Learning: Developed Temporal-Feature Cross Attention Mechanism (TFCAM), an attention-based deep learning framework that explicitly captures temporal and feature-level interactions to enhance clinical predictive modeling for Chronic Kidney Disease (CKD) progression.",
        "XAI: Provided multi-level explainability including temporal insights, feature importance ranking, and cross-temporal feature interactions, enabling clinicians to interpret model predictions transparently and identify actionable clinical insights.",
        "Enhanced Representation & Delivery: Designed an interactive, user-friendly dashboard GUI accessible to diverse user groups; integrated LLMs to automatically generate clear, insightful summaries of predictions and model interpretations, enhancing usability and clinical decision-making support."
      ],
      "startDate": "2022-07-01",
      "endDate": "2025-7-31"
    }
  ]
}
